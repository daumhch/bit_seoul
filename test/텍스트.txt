9시부터 9시까지 하기
수업이 9시 반이지만, 어쨌든 일찍 와서 미리 준비하자.

텐서플로우 2.0

자격증

2주-플젝-2주-플젝



목표:중소기업/스타트업


(레거시)머신러닝 / 딥러닝






1. 아나콘다 다운로드
https://repo.anaconda.com/archive/Anaconda3-2020.07-Windows-x86_64.exe

2. 그래픽 드라이버 최신버전 다운로드 ( RTX 2080 ) (457.09)
https://kr.download.nvidia.com/Windows/456.71/456.71-desktop-win10-64bit-international-nsd-dch-whql.exe

3. Cuba 10.1 다운로드
https://developer.nvidia.com/cuda-10.1-download-archive-update2?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal

4. Cudnn v7.6.5 for CUDA 10.1 다운로드
회원 가입 필수. daumhch@naver.com / 마이클1!

5. Visual Studio Code 다운로드




장비빨이 필요하다.
1060이상. GPU 필수.

발표: 나이, 이름, 개발스킬, 했던 일

선생님:윤영선

회사 = 시간을 돈으로 바꾸는 곳




https://www.tensorflow.org/


링크드인 계정 만들기


파이썬에서 리스트는 완벽하게 이해해야 한다.

조건문/반복문

함수와 클래스

이정도는 마스터 하자.



케라스 강좌
https://www.youtube.com/channel/UCvjXlZjlyAp2uZusgDn8lxA/videos

인공지능 블로그
https://blog.naver.com/gema0000



1. 아나콘다 설치하기

아나콘다 설치하면 파이썬+API들이 자동 설치된다.
왠만하면 아나콘다를 설치하자.
설치 폴더는 C:\Anaconda3
Path까지 추가하는거 체크하고 설치하자.


파이썬 3.8.3 사용


2/3/4 -> GPU를 사용하기 위해서.

2. 그래픽카드 설치 -> 사용자 정의 -> 그래픽 드라이버만 설치

3. CUDA 설치 -> 첫번째거 '+'눌러서 Visual Studio Integration 체크 해제

4. cudnn 압축 풀고 
-> C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1 에 덮어쓰기


###숙제 : 설치하고 스크린샷 찍어서 보내기
nvida를 쓰지 않으면 2,3,4를 빼고.


conda env list
설치된 리스트 확인


텐서플로우 설치하기
pip install tensorflow-gpu==2.3.0

cpu gpu 두 버전이 있지만, 2.3.0에서는 tensorflow로만 설치하면 둘 다 설치.


python에서,
import tensorflow as tf 하면 에러가 날 것이다.

https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads
에서 x64 버전을 설치하면, 정상 실행이 될 것이다.
(Successfully opened dynamic library cudart64_101.dll)


텐서플로우 1.0이 어려워서 나온 게 케라스(keras) API

1.0 -> 2.0 
텐서플로우 안에 keras 포함.
내부 포함이니 속도가 더 빨라짐.


케라스 설치하기
pip install keras==2.4.3

텐서플로우 확인하기
import tensorflow as tf
import keras

###숙제: numpy ..?


###숙제: 케글 가입하고 반장에게 전달.
가입하기
1. kaggle  https://www.kaggle.com/BaeHyunChul
2. DACON
3. github -> bit_seoul 저장소 만들어서 Study와 같게


==이론수업
인공지능 >= 머신러닝 > 딥러닝


뇌의 신경망

머신러닝+인공신경망 = 딥러닝

노드 = 신경망 안에 하나의 지점
레이어 = 신경망 세로 한 줄의 단계

신경망 구성하는 거 = 모델링

y=ax+b

lost / cost

훈련구간 = '정제된 데이터' x와 y를 넣는다

y=ax+b -> y=wx+b (w=weight)

머신은 '최적의' w를 구한다. (b는 영향이 작다)

또한 머신은 '최소의' loss가 되는 계산을 한다.


숙제 : acc가 왜 0.2일까, 1로 맞춰보자.


딥러닝에서,
노드의 개수와 레이어 깊이,
epochs, batch_size 등은
개발자가 정한다.

하이퍼 파라미터 튜닝 = 노드와 레이어, epochs, batch_size 조정





201110
입력레이어
히든레이어
출력레이어

하이퍼 파라미터 튜닝은, 히든레이어 조정하는 것

훈련데이터와 평가데이터는 나뉘어야 한다

정제된 데이터 = 전처리 하지 않은 데이터

최적의 weight

최소의 loss

MSE = mean_squared_error

MAE = mean_absolute_error

실제값과 예측값의 차이를 평가할 때,
음수값을 대응하기 위해 MSE 또는 MAE를 사용한다

batch_size가 크면 일괄처리 하니까 속도가 빠르다
너무 크면 제대로 된 훈련이 안 된다
(=그래픽카드 메모리가 넘친다)
batch_size도 최적화가 필요하다


loss 평가 방법
0.1 vs 0.01
낮으면 낮을수록 좋은데,
객관적인 수치는 없다


accuracy가 0.5이하는 의미 없다
할 필요가 없다


1과 0.999~~를 다르게 생각해야 한다
그래서 선형회귀에서는 accuracy를 쓸 수 없다

딥러닝 평가방식
1. 회귀 = 결과 값이 숫자로 나올 때
2. 분류 = 결과 종류가 정해져 있을 때


ctrl+/ 라인 주석
ctrl+c 라인 복사
shift+del 라인 삭제


평가 지표 중 loss는 default,
나머지는 metrics에서 추가하면 나온다

model.evaluate의 return은 list이다

metrics=['mse']를 하면
loss와 같은 값이 나온다


실습:결과물 오차 수정 및 미세조정
정답은 없지만 최적은 있다


소스 수정없이 실행했는데,
매번 결과가 다른 건 당연하다

실행할 때마다 accuracy가 
1. 1->0.5->1->0.5인 경우
2. 0.7->0.7->0.7->0.7인 경우
2번이 더 안정적인 성능을 보인 것


RMSE 지표


사이킷런


R2 결정계수
회귀분야에서 accuracy 대신 쓰는 지표


loss, RMSE, R2 항상 같이 다닌다


레이어구조는,
사각박스이거나,
다이아몬드 모양이거나,
역삼각형 모양이, 그나마 낫다

튜닝은 계속 해봐야 한다


validation(검증)
머신에게 훈련시킬 데이터의 일부를,
검증용으로 빼는 것

fit에다가 '변수_val'을 추가
또는 validation_split을 지정(퍼센트를 소수점으로)


파이썬에서 array 자르는 것 잘 기억해두자
x_train = x[:60] # 60개 
여기서 60은 값이 아니라 인덱스다

y = np.array(range(101, 201))인 경우
y_train = y[:60] 이렇게 해야 60개를 자른다


훈련데이터:검증데이터:평가용데이터 = 6:2:2


train_test_split으로 나누면,
데이터가 섞여서 분리된다 (순차적이지 않다)
단 설정된 비율은 지키면서 분리된다


순차적으로 분리하고 싶으면,
shuffle=False 옵션을 추가한다








입력이 N개가 될 수 있고,
출력도 N개가 될 수 있다

날씨/온도/습도 가지고 날씨/온도/습도 를 예측할 수 있다


column = 열
row = 행


array 객체 하나하나 = 스칼라

(30,) = 스칼라 30개, 1차원

벡터 = 스칼라의 배열, 1차원

행렬 = 2차원

텐서 = 다차원





(30,) = 30개의 스칼라가 모여있는 것
(30,1) = 30행 1열

[[1,2,3],[4,5,6]] = 2행 3열 = (2,3)

[[1,2,3,4,5,6]] = 1행 6열 = (1,6)

[[1,2],[3,4],[5,6]] = 3행 2열 = (3,2)

[[[1,2,3],[4,5,6]]] = 1x2x3 = 면x행x열
[1,2,3,4,5,6] = 6열 = (6,)

행렬 공부하자



위키독스
딥 러닝을 이용한 자연어 처리 입문 08. 딥 러닝(Deep Learnin ... 6) 케라스(Keras) 훑어보기

https://wikidocs.net/32105





201111

X-Y의 상관관계는 중요한가?
주식/채권/환율 - 기온 관계를 모델링을,
'할 수 있다'

데이터만 준비되어 있다면,
무엇이든 모델링을 할 수는 있다
평가지표가 나쁠 수는 있어도

1차원
y = w*x+b

다차원
y1, y2, y3... = w1*x1 + w2*x2 + ... + b



### 행무시 열우선
= 중요한 건 '열(column)'
= 행은 단지 나열되었을 뿐



보편적이고 일반적인건
x가 여러개 y는 하나



별도의 이야기가 없어도,
R2 높이고,
RMSE 낮추고,
loss 낮춰야 한다 = 튜닝은 꾸준히




### mlp할 때 수동으로 슬라이싱 연습하자
수동으로 자를 때에도,
다차원 배열이라 하더라도
슬라이스가 자동으로 된다



# 앞으로는 input_shape를 사용한다



validation을 넣으면서,
epoch 출력 결과에
loss와 val_loss 차이가 있다

val_loss를 통해 최적 epoch를 구할 수 있다



==== Sequential 기초 끝 ====




함수형 모델

함수를 사용하는 목적: 재사용



activation = 활성화 함수
레이어 전달 할 때,
문제 있는 값을 정리하는 기능
모든 레이어 마다 있다
Dense layer의 default activatin은 'linear'
'relu'를 사용하는 게 무난하다
마지막 activation은 'linear' 해야 한다




시퀀셜 모델은,
모델 정의하고 구성했다면,

함수형 모델은,
구성을 먼저하고 모델을 정의한다


model.summary()에서
모델을 확인했을 때,
param의 결과가
배치된 layer 및 node 개수와 다른 이유는,

node 개수에 바이어스노드(+1) 해주기 때문



### 내 생각
https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks

y=w*x+b = w*x+b*1

모델 안에는 연산이기 때문에,
b 또한 b*1로 생각하여 연산한다
그래야 찾을 수 있을 것 같다

### 내 생각 끝


함수형 모델의 summary는
Model: "functional_1"
이런 형태로 메시지가 나온다




### 앙상블(ensemble)

모델의 신뢰성 이전에,

데이터 신뢰성은 어떻게?

=도박사의 오류=
서로 영향을 끼치지 않는 일련의 확률적 사건들에서 
상관관계를 찾아내려 하는 사고의 오류를 이야기한다.



데이터를 합치지 않고,
데이터별로 모델을 만들어서,
모델을 합친다 = concatenate


input model 2

middle 1

output model 2

총 5개




https://keras.co.kr/
선생님 홈페이지

https://keras.io/
참고 페이지

이름붙이고 싶으면 Dense 파라미터 name을 사용하자


Concatenate와 concatenate는 다르다
Concatenate를 사용할 때에는 axis를 설정해야 한다
ex) keras16_ensemble.py
0 = 가장 높은 차원 기준으로 합치기 -> Total params = 2323
1 = 두번째로 높은 차원 기준으로 합치기 -> Total params = 2413
-1 = 가장 낮은 차원 기준으로 합치기 -> Total params = 2413




숙제: 앙상블 예제 3번 튜닝하기


내일은 LSTM


201112

복습
시퀀셜 -> 함수형 모델
모델 섞기 -> 앙상블
튜너


오늘은 LSTM 할 예정

DNN = deep neural networks
RNN = recurrent neural networks 
- LSTM = Long Short-Term Memory models

RNN = 순환 신경망 = 순차적인 데이터 분석 = Time Series = 시계열
시계열이라 하여 주식/온도 처럼 시간데이터 뿐 아니라,
'나는 밥을 먹는다'처럼, 텍스트 또한 순차적인 의미가 있다
자연어 처리, 챗봇의 기초


1, 2, 3, 4 라는 데이터에서,
다음 값이 5라고 예상되는 이유는, weight와 bias를 경험적으로 계산했기 때문에


LSTM = 가장 좋은 성능의 RNN
https://gruuuuu.github.io/machine-learning/lstm-doc2/#

GRU = LSTM을 더 발전시킨 구조


LSTM 파라미터 계산 방법
https://en.wikipedia.org/wiki/Long_short-term_memory#/media/File:LSTM_cell.svg

4*(n+m+b)*m
4 = sigmoid 3개 + tanh 1개
n = input vector
m = LSTM layer number
b = bias = 1


내 생각에,
feedback이 아니고 recurrent인 이유: 값 + 상태를 되먹인다


input_shape = (input_length , input_dim)



simpleRNN은,
(n+m+b)*m = parameter의 수

GRU는,
### 조사하자



하이퍼파리미터 튜닝 조건 추가 : EarlyStopping에서 patience 지표



